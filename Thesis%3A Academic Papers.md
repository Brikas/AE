- ✔ 🔸 [[LLMs as Simulated Economic Agents: Homo Silicus (Horton, 2023)]] #paper
  id:: 65ecc4f4-b5af-4f3b-b286-3b8787ec422e
	- {{embed [[LLMs as Simulated Economic Agents: Homo Silicus (Horton, 2023)]]}}
- ✔📚 🚀🔹🔸 [[Using GPT for Market Research (Brand et al., 2023)]]
  id:: 65af92bb-da88-45b8-90af-ca5770651146
  collapsed:: true
	- {{embed [[Using GPT for Market Research (Brand et al., 2023)]]}}
- ✔📚 Using Large Language Models to Simulate Multiple Humans. Aher, Arriaga and Kalai (2023), #paper
  id:: 65eace94-9e44-44c1-bc48-47651188e3c2
  collapsed:: true
	- [PDF](G:/My Drive/CBS/Thesis/Using Large Language Models to Simulate Multiple Humans.pdf)
	- Refs
	  collapsed:: true
		- From ((65af92bb-da88-45b8-90af-ca5770651146))
			- Aher et al. (2022) simulate psychological studies including the well-known Milgrom shock experiment.
		- Cited by 150
	- simulating a representative **sample** of participants
	- Classic **economic**, **psycholinguistic**, and **social psychology** experiments
		- Ultimatum Game
		- Garden Path Sentences
		- Milgram Shock Experiment
		- Wisdom of Crowds
	- 🔸 Classification vs Simmulation prompts
		- Class
		  collapsed:: true
			- ![](https://img001.prntscr.com/file/img001/ij-rEQIdSRCcjggmFfPIKw.png)
		- Sim
			- https://img001.prntscr.com/file/img001/P65OOavnQ42AqEnLD0ZPPg.png
			- Can be used to simulate the responses of multiple different individuals by varying the name.
	- Prompt engineering
		- 🔸 Chain-of-thoughts
		  id:: 6605ce4a-9c73-4954-a461-611e8da7b3a8
			- Thinking out loud.
	- Data
		- Salary
		- Age
		- etc.
		- Name
			- from a pool
	- Validity ❓
	- Random
		- ![](https://img001.prntscr.com/file/img001/I9M_fvh4Qamm0pwcOstuow.png)
		-
- ✔📚 Can AI language models replace human participants? (Dillion et al., 2023) #paper
  id:: 66157a3e-4f6c-4490-bb7f-a4a51c3959b0
  collapsed:: true
	- Meta
		- [PDF](G:/My Drive/CBS/Thesis/Can AI language models replace human participants.pdf)
		- [lnk](https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(23)00098-0)
		- 110 citations
	- https://nikett.github.io/gpt-as-participant/
	- synthetic AI participants
		- https://www.syntheticusers.com/
	- Topics
	  collapsed:: true
		- [[Self-refinement]]
			- Self-Refine iteratively improves outputs from LLMs through a process of iterative creation with feedback description.
			- https://selfrefine.info/
				- ![](https://img001.prntscr.com/file/img001/RaogXqwoRaiz8M0WYb9gHA.png)
	- The study
	  collapsed:: true
		- > We investigated the correspondence between average human judgments and GPT-3.5 (text-davinci-003)’s judgments on 464 moral scenarios from five published papers [5–9].
			- Perhaps we can replicate with out own data?
			- {{embed ((6605ce4a-0d1e-4b4c-a50c-fc44e1818918))}}
		- Data
			- Data for replication https://nikett.github.io/gpt-as-participant/
			  id:: 6605ce4a-0d1e-4b4c-a50c-fc44e1818918
			- Clifford et al. (2015)
			- Cook & Kuhn (2021)
			- Effron (2022)
			- Grizzard et al. (2021)
			- Mickelberg et al. (2022)
		- Results: Striking Correlation
		  collapsed:: true
			- ![](https://img001.prntscr.com/file/img001/N20fp5pKTUG-SqVOQdBj-w.png){:height 259, :width 385}
			-
	- Behavior vs language.
		- LLM are solution for language.
		- Some questions are better studied by behavior, some by language
	- Refs
		- > predicting voting choices
			- ((660accde-ab83-44f8-97a8-5f9c4fde0676))
		- > replicating behavior in economic games
			- ((65ecc4f4-b5af-4f3b-b286-3b8787ec422e))
		-
- ✔📚 🔹 [[InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews (Wang & Xiao et.al, 2023)]] #paper #methodology
  collapsed:: true
	- {{embed [[InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews (Wang & Xiao et.al, 2023)]]}}
- ✔📚 🔹 Evaluating and Inducing Personality in Pre-trained Language Models (2023) #4Airidas
  id:: 66000105-c62a-4788-b012-1fc251d169f4
  collapsed:: true
	- [pdf](G:/My Drive/CBS/Thesis/NeurIPS-2023-evaluating-and-inducing-personality-in-pre-trained-language-models-Paper-Conference.pdf)
	  collapsed:: true
	- Trivia
	  collapsed:: true
		- [[Reinforcement Learning from Human Feedback]] (RLHF)
		- Figure
		  collapsed:: true
			- ![](https://img001.prntscr.com/file/img001/AklcbWfsRLyQzfzAJh5PEA.png)
			-
		- Related Work
		  collapsed:: true
			- Personality and Language - [[Big Five]] x Language or [[16PF]] x Language
			-
			-
		- [[human personality theory]]
		- [lnk](https://proceedings.neurips.cc/paper_files/paper/2023/hash/21f7b745f73ce0d1f9bcea7f40b1388e-Abstract-Conference.html)
		- 40 cits
		- [[psychometric]]
	- Evaluating machines’ personality
	  background-color:: yellow
		- [[Machine Personality Inventory (MPI)]]
		  collapsed:: true
			- Systematic and Quantitative theory of [[machine personality]]
			- [[Big Five Personality Factors]] (Big Five)
			  id:: 6601cb0b-fad8-4c33-aefc-e706d10ca83e
			  collapsed:: true
				- *McCrae and John (1992)*
				- [[psychometric]]
				- Five
					- [[Openness]]
					  collapsed:: true
						- artistic, curious, imaginative, insightful, and original with wide interests.
					- [[Conscientiousness]]
					  collapsed:: true
						- efficient, organized, planful, reliable, responsible, and thorough.
					- [[Extraversion]]
					  collapsed:: true
						- active, assertive, energetic, enthusiastic, outgoing, and talkative.
					- [[Agreeableness]]
					  collapsed:: true
						- appreciative, forgiving, generous, kind, and sympathetic.
					- [[Neuroticism]]
					  collapsed:: true
						- anxious, self-pitying, tense, touchy, unstable, and worrying
			- TODO [[International Personality Item Pool (IPIP)]] ❓
			  id:: 6601d051-7a32-4faa-9bd6-3181ca25ef61
			  collapsed:: true
				- *(Goldberg et al., 1999, 2006; Johnson, 2005, 2014)*
				- *Lang et al. (2011)’s BFI-S*
				-
			- Each MPI item consists of
				- a question
					- asks the machine to evaluate the degree of fitness of a self-description
				- a set of options.
			- 🔸 **MPI dataset** Example (its like pieces of modules)
			  collapsed:: true
				- ![](https://img001.prntscr.com/file/img001/4lQWF4iASlSuiSdlI4-mRw.png)
				  id:: 6601d155-f902-4de9-bf7f-e4d9c8c5ebdc
		- 🔸 Results
		  collapsed:: true
			- ![](https://img001.prntscr.com/file/img001/PGCwY7pITcycSWW0-fg3cQ.png)
			-
	- 🔹 Inducing LLMs’ Personality - ((66000105-916b-4588-b401-72b81acdd3e3)) & ((65ff2f20-285f-4826-97e1-12a518530483))
	  background-color:: yellow
		- 🔸 PERSONALITY PROMPTING (P2) method
			- Zero-shot prompting ❓
			- Words Auto Prompting ❓
			- Trivia
			  collapsed:: true
				- induce LLMs with specific personalities in a **controllable** way
				- On MPI evaluation and human vignette tests, the P2 method yields high efficacy in personality induction.
			- Figure
			  collapsed:: true
				- ![](https://img001.prntscr.com/file/img001/YNbBiKj5QzCyajaGdE3rTQ.png){:height 261, :width 658}
			- Uses
			  collapsed:: true
				- {{embed ((6601cb0b-fad8-4c33-aefc-e706d10ca83e))}}
			- Results P2 is superior and more consistent than Naive and Words Auto Prompting
			- Vignette test Results
			  collapsed:: true
				- ![](https://img001.prntscr.com/file/img001/aNqo0B9aQU6kZhStJK9roQ.png)
			-
		- Chain-of-thought prompting method (Wei et al., 2022b)
		- 🔸 {{g Personality}} - a collection of consistent behaviors
		  id:: 66031379-0c6b-4d49-ab7b-f83e3ded3a4c
	- Other Asssets
		- 🔸 ((6601d330-7936-4411-a312-35e608c0beb1))
		  collapsed:: true
			- ((66031379-0c6b-4d49-ab7b-f83e3ded3a4c))
			- collapsed:: true
			  > For instance, a model that yields precisely the same response to all questions (e.g., all A in [Tab. 1](((6601d155-f902-4de9-bf7f-e4d9c8c5ebdc)))) will inevitably produce high-variance results due to the positively and negatively related items, invalidating any signal of a stable personality.
				- ((6601d155-f902-4de9-bf7f-e4d9c8c5ebdc))
		- ((66000105-1fd1-456f-8ad4-12164a7e6b2c)) (maybe)
		  collapsed:: true
			- TODO Johnson (2014)’s 619,150 human responses on the IPIP-NEO-120 inventory
			  id:: 6601ccd7-b6af-4bf8-a0f1-ef2b7b023b60
- ✔📚 Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment (2024) #4airidas
  collapsed:: true
	- [pdf](G:/My Drive/CBS/Thesis/Large Language Models are Superpositions of All Characters Attaining.pdf)
	- Keeping notes only highliter #[[Note-taking]]
	  collapsed:: true
		- Reflection
			- Some things, I want to write down like 'focused on open-source element'
	- TODOs
	  collapsed:: true
		- lOok at refs
	- Concepts
		- [[role-play]]
		- summoning **intrinsic** character knowledge
	- ((65eccdd0-c8f6-4f02-9e1f-1f83b760a048))
		- 🔸 [[self-alignment]] - LLMs as Judges
			- An emerging method to cheaply improve a weaker language model is to fine-tune it on outputs from a stronger model, such as a proprietary system like GPT-4.
			- Li et al. (2023b) utilized **the model itself** to both augment and curate high-quality training examples, enhancing its own performance
	- Metric Design
		- Consistent Role Identity
		  logseq.order-list-type:: number
		- Accurate role-related knowledge
		  logseq.order-list-type:: number
		- Reject unknown questions
		  logseq.order-list-type:: number
	- Supervised Finetuning
		- Finetune the LLM on the self-generated dataset to inject role-play capabilities
		- During the finetuning, we remove the injected knowledge and only retain a very brief introduction of the character.
		  id:: 66069114-70c3-4fe3-928f-80d85c0c15ff
	- Cross Supervision
		- Different models for generating dialogue and judging.
- ✔📚 [[Argyle, L.P. et al. (2023) Out of one, many: using language models to simulate human samples]]. Polit.
  id:: 660accde-ab83-44f8-97a8-5f9c4fde0676
  collapsed:: true
	- {{embed [[Argyle, L.P. et al. (2023) Out of one, many: using language models to simulate human samples]]}}
- TODO 🔸 Li, P., Castelo, N., Katona, Z., & Sarvary, M. (2023). Language models for automated market research: A new way to generate [[perceptual map]]s. Available at SSRN 4241291. #branding
  id:: 660ae28c-e26a-4e02-a721-1c20ad247993
  collapsed:: true
	- [PDF - Win](G:/My Drive/CBS/Thesis/Determining the Validity of Large Language Models for Automated Perceptual Analysis.pdf)
	  collapsed:: true
		- [lnk](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4241291)
		- 8 cits
		- a.k.a: Determining the Validity of Large Language Models for Automated Perceptual Analysis
	- This paper explores the potential of Large Language Models (LLMs) to substitute for human participants in market research.
	- perceptual analysis
		- for certain product categories
		- brand similarity measures
		- product attribute ratings.
	- agreement rates between human- and LLM- generated data sets reach over 75%
	- Language analysis x [[Market Research]]
	- ((65ff2f20-285f-4826-97e1-12a518530483))
		- **“The car brand BMW is similar to the car brand .....”**, where the model fills in the blank.
			- We then search for the mentions of other brands in the output, such as “Audi” or others.
			- After rigorous analysis of the frequencies of brands appearing after repeated prompting, we can generate similarity scores between pairs of brands.
		- Time of the inquiry
			- **injecting a designated year into the prompt.**
		- 🔸 baseline bias
			- > However, these frequencies can be biased by brands’ top-of-mind awareness or other brand-specific features that affect the overall mentions of the brand. We call this bias the baseline value of a brand.
			- **ordinal embedding method**
				-
	- ((65c3f123-26dd-4fb9-899f-839c9376ce1a))
		- [[Perceptual Analysis]]
		  collapsed:: true
			- Types
				- relying on brand similarities
				  logseq.order-list-type:: number
					- [[EvoMap]]
						- Matthe, Ringel, and Skiera (2022)
					- t-distributed Stochastic Neighborhood Embedding (t-SNE)
				- based on the most important **attributes and features** that contribute to a brand’s perceived image
				  logseq.order-list-type:: number
					- (e.g. safety, economy, sportiness, etc., for cars).
			-
	- ((66000105-916b-4588-b401-72b81acdd3e3))
		- Basic characteristics from Survey data
			- age
			- gender
			- income
	- Random
		- user-generated content (UGC) to infer competitive market structures
			- Netzer et al. (2012), that applies text-mining techniques and semantic network analysis to comments on a public forum to deduce the market structure of sedan cars and diabetes drugs.
			- Using a much broader set of (200) brands, Culotta and Cutler (2016) use UGC data on Twitter to infer attribute-specific brand perceptions
			-
- TODO (for data) Debunking Misinformation About Consumer Products: Effects on Beliefs and Purchase Behavior.  Fong et al. (2023). #paper
  id:: 660adf2c-b555-43eb-aa2d-8e0cf3a54f0f
	- [pdf](G:/My Drive/CBS/Thesis/fong-et-al-2023-debunking-misinformation-about-consumer-products-effects-on-beliefs-and-purchase-behavior.pdf)
- TODO Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs (2024)
  collapsed:: true
	- [lnk](https://arxiv.org/abs/2311.04892)
	- 6 cits
	- Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups
	- For [[discussion]]
- TODO AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback (2023)
  collapsed:: true
	- [lnk](https://proceedings.neurips.cc/paper_files/paper/2023/hash/5fc47800ee5b30b8777fdd30abcaaf3b-Abstract-Conference.html)
	- 146 citations
- Refferenced
  id:: 6605ce4a-acfd-453a-8533-5ce475eae59b
  collapsed:: true
	- ((65ff2f20-285f-4826-97e1-12a518530483))
		- > chain prompts can affect LLMs’ behaviors better than examples (Wei et al., 2022b)
			- from
			  collapsed:: true
				- ((66000105-c62a-4788-b012-1fc251d169f4))
- Mapped (sorted by citations)
  id:: 65f354a5-9909-4d53-b940-a214a0ae479c
  collapsed:: true
	- Using cognitive psychology to understand GPT-3 (2022, June). Marcel Binz, Eric Schulz
	  collapsed:: true
		- 250 cits
		- [url](https://arxiv.org/abs/2206.14576)
	- ((65fc9d59-c33c-4085-bac1-ecc5bae87dbc))
	- Challenges and Applications of Large Language Models (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2307.10169)
		- 140 citations
	- Whose Opinions Do Language Models Reflect? (2023)
	  collapsed:: true
		- [lnk](https://proceedings.mlr.press/v202/santurkar23a.html)
		- 125 citations
	- A Survey on Large Language Model based Autonomous Agents (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2308.11432)
		- 200 citations
	- The Rise and Potential of Large Language Model Based Agents: A Survey (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2309.07864)
		- 180 cits
	- In-Context Impersonation Reveals Large Language Models' Strengths and Biases (2023)
	  collapsed:: true
		- [lnk](https://proceedings.neurips.cc/paper_files/paper/2023/hash/e3fe7b34ba4f378df39cb12a97193f41-Abstract-Conference.html)
		- 40 cits
	- Using large language models in psychology (2023)
	  collapsed:: true
		- [lnk](https://www.nature.com/articles/s44159-023-00241-5)
		- 35 cits
	- Playing repeated games with Large Language Models (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2305.16867)
		- 50 cits
	- S3: Social-network Simulation System with Large Language Model-Empowered Agents (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2307.14984)
		- 20 cits
	- Can Generative AI Improve Social Science?
	  collapsed:: true
		- 25 cits
		- [lnk](https://osf.io/preprints/socarxiv/rwtzs/)
	- 🔸 CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2310.11501)
		- 11 cits
	- Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding (2024)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2401.12954)
		- 5 cists
	- Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination (2023) #[[Fine tuning]]
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2309.11696)
		- 6 cits
	- GPT and CLT: The impact of ChatGPT's level of abstraction on consumer recommendations (2024)
	  collapsed:: true
		- [lnk](https://www.sciencedirect.com/science/article/pii/S0969698923003314)
		- 3 cits
	- CGMI: Configurable General Multi-Agent Interaction Framework (2023)
	  collapsed:: true
		- [lnk](https://arxiv.org/abs/2308.12503)
- Mapped (pre-GPT or non-AI)
  collapsed:: true
	- Netzer, O., Feldman, R., Goldenberg, J., & Fresko, M. (2012). Mine your own business: Marketstructure surveillance through text mining. Marketing Science, 31(3), 521–543.
	  id:: 660ae25c-baa5-46d0-9547-33d00a1ef4c3
- Irrelevant
  background-color:: gray
  id:: 65ecdee6-8ddf-4f95-8ee4-3a7d0e370e9a
  collapsed:: true
	- ✔ Can AI serve as a substitute for human subjects in software engineering research? (2024) #4Airidas
	  collapsed:: true
		- [pdf](G:/My Drive/CBS/Thesis/Can AI Serve as a Substitute for Human Subjects.pdf)
		  collapsed:: true
			- [lnk](https://arxiv.org/abs/2311.11081)
		- 1 cit
		- Contents
		  collapsed:: true
			- AI-Based Foundation Models as Alternates to Human Data Sources
			  logseq.order-list-type:: number
				- Interviews: persona-based prompting
				- Focus Groups: multi-persona prompting
				- Surveys: mega-persona prompting
				- Beyond textual data: observation and user experiments
			- Open Problems and Research Opportunities
			  logseq.order-list-type:: number
		- Qualitative Data
		- ((65ff2f20-285f-4826-97e1-12a518530483))
			- Very basic. Everything is one prompt. No simulation of any kind. Even multi samples are put into one
	- Karolina's Thesis
	  collapsed:: true
		- [PDF File](G:/My Drive/CBS/Thesis/1645570_Master_Thesis_Lambru_Kaszkowiak_Khalid_2023.pdf)
		- [Online Link](https://research.cbs.dk/en/studentProjects/improving-user-satisfaction-in-university-affiliated-incubators-a)
		- Methodology
			- [[Kano Model Mapping]]
			  collapsed:: true
				- Applying the Kano model to the quantitative data collected from the survey to **categorize different types of customer needs** and prioritize these factors in the design of startup incubators. Develop an instrument based on Excel formula to assign attributes and calculate Satisfaction Score and Dissatisfaction Score.
				- [[Kano Model Mapping]] #embed-node
				  collapsed:: true
					- {{embed [[Kano Model Mapping]]}}
			-
		-
	- 👎 GPT revolution and digital entrepreneurial intentions
	  id:: 65eace94-5312-4326-88ac-eaee838f67c9
	  collapsed:: true
		- [lnk](https://www.emerald.com/insight/content/doi/10.1108/JEEE-07-2023-0260/full/html)
		- References ((65af92bb-da88-45b8-90af-ca5770651146))
	- Modelling and Simulation of Human-Environment Interactions (book) by *Philippe J Giabbanelli*
	  collapsed:: true
		- [lnk](https://www.amazon.com/Modelling-Simulation-Human-Environment-Interactions-Giabbanelli/dp/3036528083)
		- ![image.png](../assets/image_1706005062674_0.png){:height 341, :width 733}
		- .
-